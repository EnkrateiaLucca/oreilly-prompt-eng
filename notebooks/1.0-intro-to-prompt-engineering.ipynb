{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Basics\n",
    "\n",
    "A prompt is a piece of text that conveys to a LLM what the user wants. What the user wants can be many things like:\n",
    "\n",
    "- Asking a question\n",
    "- Giving an instruction\n",
    "- Etc...\n",
    "\n",
    "The key components of a prompt are:\n",
    "1. Instruction: where you describe what you want\n",
    "2. Context: additional information to help with performance\n",
    "3. Input data: data the model has not seem to illustrate what you need\n",
    "4. Output indicator: How you prime the model to output what you want, for example asking the model [\"Let's think step by step\" and the end of a prompt can boost reasoning performance](https://arxiv.org/pdf/2201.11903.pdf). You can also write \"Output:\" to prime the model to just write the output and nothing else.\n",
    "\n",
    "[Prompts can also be seen as a form of programming that can customize the outputs and interactions with an LLM.](https://ar5iv.labs.arxiv.org/html/2302.11382#:~:text=prompts%20are%20also%20a%20form%20of%20programming%20that%20can%20customize%20the%20outputs%20and%20interactions%20with%20an%20llm.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on how to present:\n",
    "\n",
    "- Show yourself giving just an instruction to ChatGPT\n",
    "- Then add context\n",
    "- Then add separation between context and input text\n",
    "- Then finally add output indicator\n",
    "\n",
    "- Interesting strategies\n",
    "- General rules for incorporating strategies \n",
    "- Experiment Template\n",
    "    - When to experiment and when to just use what works\n",
    "- Storing mistakes and errors for improvement\n",
    "- Prompt management strategies for everyone (developers and non-developers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Data & Context Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"Text: I like eating pancakes.\"\n",
    "\n",
    "context = \"You are a text annotation engine.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_indicator = \"Output:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How you ask what you want, and the heavily relies on what you want from the model.\n",
    "# Instruction prompt: \n",
    "\n",
    "instruction_prompt = f\"{context}.\\n{instruction}.\\n{input_data}. {output_indicator}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a text annotation engine..\\nClassify this sentence into positive or negative:.\\nText: I like eating pancakes.. Output:\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instruction_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Output: Positive'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(prompt_question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "get_response(instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's a joke for you: Why don't scientists trust atoms? Because they make up everything!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def get_response(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # describes overall behavior\n",
    "            {\"role\": \"user\", \"content\": prompt} #message from the user\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "prompt = \"Tell me a joke\"\n",
    "get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Strategies\n",
    "\n",
    "- Strategy 1: Write clear instructions\n",
    "- Strategy 2: Provide reference text\n",
    "- Strategy 3: Break tasks into subtasks\n",
    "- Strategy 4: Give the model time to think\n",
    "- Strategy 5: Use external tools\n",
    "- Strategy 6: Test changes systematically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Strategy 1: Write clear instructions\n",
    "system_message = \"You are a helpful assistant\"\n",
    "clear_instructions_prompt = \"What is the capital of Canada?\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": clear_instructions_prompt}\n",
    "])\n",
    "print(\"Strategy 1 - Clear Instructions:\\n\", response.choices[0].message.content)\n",
    "\n",
    "# Strategy 2: Provide reference text\n",
    "system_message = \"You are an expert AI researcher\"\n",
    "reference_text_prompt = \"\"\"The following is a research paper:\n",
    "'''\n",
    "'''\n",
    "Answer the following questions based solely on the contents of this paper.\n",
    "\n",
    "1. Explain positional encoding intuitively\n",
    "2. How does self-attention allows the arctheicture to understand context in a long sentence?\n",
    "\n",
    "Your asnwers:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategy 2 - Provide Reference Text:\n",
      " 1. Positional Encoding Intuitively:\n",
      "Positional encoding in the Transformer architecture serves as a mechanism to provide the model with the sequential order information of the input tokens, which is crucial for understanding the structured nature of language. Since the Transformerâ€™s self-attention mechanism treats input elements independently without considering their positions in the sequence, it inherently lacks the means to capture the order of tokens, which is vital for comprehending many linguistic constructs. \n",
      "\n",
      "To address this, positional encodings are added to the input embeddings, ensuring that the position information is available to the model right from the start. The use of sine and cosine functions of different frequencies as positional encodings is particularly insightful because it provides a unique positional signal for each token while also enabling the model to understand relative positions. The choice of these functions allows the model to easily learn to attend by relative positions, as the positional encoding for any given position can be represented as a linear function of the encodings of other positions. This property is beneficial for tasks involving sequences where the relative positioning of elements significantly impacts their interpretation, such as in language understanding.\n",
      "\n",
      "2. Understanding Context in Long Sentences through Self-Attention:\n",
      "Self-attention, a key innovation in the Transformer architecture, enables the model to efficiently handle long-range dependencies within a sentence. Unlike traditional recurrent or convolutional layers that process sequences step-by-step or within local neighborhoods, self-attention allows each token in the input sequence to directly attend to every other token. This is accomplished by computing the attention weights that reflect the relevance or contribution of all other tokens with respect to a given token.\n",
      "\n",
      "This mechanism provides a couple of significant advantages for understanding context in long sentences:\n",
      "\n",
      "- **Direct Modeling of Relationships**: Self-attention directly models the relationships between all pairs of tokens in a sentence, regardless of their distance in the sequence. This capability is crucial for understanding the context, as it allows the model to capture dependencies between distant tokens without the information needing to traverse through many intermediate steps, as in recurrent networks. Consequently, it effectively deals with long-range dependencies that are common in natural language, enhancing the model's ability to understand the overall context of long sentences.\n",
      "\n",
      "- **Parallel Processing**: Self-attention enables parallel processing of all token pairs, making the model computationally efficient, especially for long sentences. This efficiency arises because the computation of attention weights for all pairs occurs simultaneously, instead of sequentially. The ability to handle all parts of the sentence concurrently improves the modelâ€™s understanding of context, as it can consider the entire sentence's information at once, rather than piecemeal.\n",
      "\n",
      "In summary, self-attention gives the Transformer the remarkable ability to directly and efficiently capture the complex interdependencies between tokens in long sentences, leading to a deeper and more nuanced understanding of the context.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=\"gpt-4-0125-preview\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": reference_text_prompt}\n",
    "])\n",
    "print(\"\\nStrategy 2 - Provide Reference Text:\\n\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Break tasks into subtasks\n",
    "system_message = \"You are a helpful AI assistant.\"\n",
    "subtasks_prompt = \"\"\"The following is a paper explaining this architecture named 'Transformer':\n",
    "'''\n",
    "'''\n",
    "1. Break this paper down into its main sections.\n",
    "2. Summarize each section in one sentence.\n",
    "3. Write a pargraph combining these summarizations\n",
    "4. Your output should be both the paragraph summary and the bullet points section summary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategy 3 - Break Tasks into Subtasks:\n",
      " **Main Sections:**\n",
      "1. Introduction\n",
      "2. Background\n",
      "3. Model Architecture\n",
      "4. Training\n",
      "5. Results\n",
      "6. Conclusion\n",
      "7. Attention Visualizations\n",
      "\n",
      "**Summaries:**\n",
      "1. Introduces the Transformer architecture based on attention, which outperforms RNNs and CNNs in machine translation tasks.\n",
      "2. Discusses the challenges with recurrent models, the benefits of attention mechanisms, and the motivation for the Transformer.\n",
      "3. Describes the structure of the Transformer model, including encoder and decoder stacks, attention mechanisms, and positional encoding.\n",
      "4. Details the training data, hardware setup, optimizer, and regularization techniques used for training the model.\n",
      "5. Presents the results of the Transformer model on machine translation tasks and English constituency parsing, highlighting its superior performance.\n",
      "6. Concludes by discussing the future applications of attention-based models and provides insights for further research.\n",
      "7. Displays visualizations of attention heads in the Transformer, showing how the model learns to attend to specific parts of input sequences for better understanding.\n",
      "\n",
      "**Paragraph Summary:**\n",
      "The paper introduces the Transformer architecture, a model based on attention mechanisms that outperforms traditional RNNs and CNNs in machine translation tasks. It highlights the challenges with recurrent models and emphasizes the benefits of attention mechanisms for modeling long-range dependencies efficiently. The Transformer model's structure, training process, and impressive results in machine translation, as well as in English constituency parsing, are detailed in the paper. The conclusion discusses the potential applications of attention-based models beyond text translation tasks and provides insights into future research directions. Visualizations of attention heads in the Transformer model demonstrate its ability to capture complex dependencies within sequences.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=\"gpt-4-0125-preview\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": subtasks_prompt}\n",
    "])\n",
    "print(\"\\nStrategy 3 - Break Tasks into Subtasks:\\n\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 4: Give the model time to think\n",
    "system_message = \"\"\n",
    "time_to_think_prompt = \"\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": time_to_think_prompt}\n",
    "])\n",
    "print(\"\\nStrategy 4 - Give the Model Time to Think:\\n\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 5: Use external tools (e.g., Python code execution)\n",
    "system_message = \"\"\n",
    "external_tools_prompt = \"\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": external_tools_prompt}\n",
    "])\n",
    "print(\"\\nStrategy 5 - Use External Tools:\\n\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 6: Test changes systematically\n",
    "system_message = \"\"\n",
    "test_changes_systematically_prompt = \"\"\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": test_changes_systematically_prompt}\n",
    "])\n",
    "print(\"\\nStrategy 6 - Test Changes Systematically:\\n\", response.choices[0].essage.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Prompt Engineering is a trial-and-error process that also relies on your intuition.\n",
    "\n",
    "\n",
    "Build domain understanding - use your domain expertise to customize prompt for relevance.\n",
    "\n",
    "\n",
    "Build model understanding - adapt prompt to suit model strengths & weaknesses to improve quality.\n",
    "\n",
    "Iterate & Validate - define acceptance or termination criteria so you iterate to meet expectations but don't over-engineer the prompt to reduce reusability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT](https://ar5iv.labs.arxiv.org/html/2302.11382)\n",
    "- [Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)\n",
    "- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/pdf/2107.13586.pdf)\n",
    "- [prompt engineering guide - zero shot prompting example](https://www.promptingguide.ai/techniques/zeroshot)\n",
    "- [Finetuned language models are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)\n",
    "- [prompt engineering guide - few shot prompting](https://www.promptingguide.ai/techniques/fewshot)\n",
    "- [prompt engineering guide - chain of thought prompting](https://www.promptingguide.ai/techniques/cot)\n",
    "- [Wei et al. (2022)](https://arxiv.org/abs/2201.11903)\n",
    "- [prompt engineering guide - self-consistency](https://www.promptingguide.ai/techniques/consistency)\n",
    "- [prompt engineering guide - generate knowledge](https://www.promptingguide.ai/techniques/knowledge)\n",
    "- [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf)\n",
    "- [prompt engineering guide - tree of thoughts (ToT)](https://www.promptingguide.ai/techniques/tot)\n",
    "- [Prompt Engineering by Lilian Weng](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n",
    "- [Prompt Engineering vs. Blind Prompting](https://mitchellh.com/writing/prompt-engineering-vs-blind-prompting#the-demonstration-set)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
